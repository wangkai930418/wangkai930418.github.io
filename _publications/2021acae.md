---
title: "ACAE-REMIND for online continual learning with compressed feature replay"
collection: publications
# permalink: 
# excerpt: 
date: 2021-05-18
venue: 'Pattern Recognition Letters'
paperurl: 'https://www.sciencedirect.com/science/article/abs/pii/S0167865521002312'
# citation: '@InProceedings{Yu_2020_CVPR,
# author = {Yu, Lu and Twardowski, Bartlomiej and Liu, Xialei and Herranz, Luis and Wang, Kai and Cheng, Yongmei and Jui, Shangling and Weijer, Joost van de},
# title = {Semantic Drift Compensation for Class-Incremental Learning},
# booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
# month = {June},
# year = {2020}
# }'
---
<!-- Online continual learning aims to learn from a non-IID stream of data from a number of different tasks, where the learner is only allowed to consider data once. Methods are typically allowed to use a limited buffer to store some of the images in the stream. Recently, it was found that feature replay, where an intermediate layer representation of the image is stored (or generated) leads to superior results than image replay, while requiring less memory. Quantized exemplars can further reduce the memory usage. However, a drawback of these methods is that they use a fixed (or very intransigent) backbone network. This significantly limits the learning of representations that can discriminate between all tasks. To address this problem, we propose an auxiliary classifier auto-encoder (ACAE) module for feature replay at intermediate layers with high compression rates. The reduced memory footprint per image allows us to save more exemplars for replay. In our experiments, we conduct task-agnostic evaluation under online continual learning setting and get state-of-the-art performance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset. -->

[Paper link](https://www.sciencedirect.com/science/article/abs/pii/S0167865521002312)

<!-- Recommended citation: Your Name, You. (2010). "Paper Title Number 2." <i>Journal 1</i>. 1(2). -->