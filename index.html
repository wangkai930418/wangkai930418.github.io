<!DOCTYPE html>
<!--[if IE 8]> <html lang="en" class="ie8"> <![endif]-->  
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->  
<!--[if !IE]><!--> <html lang="en"> <!--<![endif]-->  
<head>
    <title>Yaxing Wang @ PhD Student</title>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Personal website from Yaxing Wang">
    <meta name="author" content="Yaxing Wang">    
    <link rel="shortcut icon" href="favicon.ico">  
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'> 
    <!-- Global CSS -->
    <link rel="stylesheet" href="assets/plugins/bootstrap/css/bootstrap.min.css">   
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="assets/plugins/font-awesome/css/font-awesome.css">
    <!-- github acitivity css -->
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/octicons/2.0.2/octicons.min.css">
    <link rel="stylesheet" href="http://caseyscarborough.github.io/github-activity/github-activity-0.1.0.min.css">
    
    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="assets/css/styles.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    
</head>

<body>
    <!-- ******HEADER****** --> 
    <header class="header">
        <div class="container">                       
            <img class="profile-image img-responsive pull-left" src="assets/images/yaxing1.png" alt="Yaxing Wang" />
            <div class="profile-content pull-left">
                <h1 class="name">Yaxing Wang</h1>
                <h2 class="desc">PhD Candidate at Computer Vision Center</h2>   
                <ul class="social list-inline">
                    <!--<li><a href="https://twitter.com/paurif"><i class="fa fa-twitter"></i></a></li>-->
                    <!--<li><a href="https://www.linkedin.com/in/priba/"><i class="fa fa-linkedin"></i></a></li>-->
                    <li><a href="https://github.com/yaxingwang"><i class="fa fa-github"></i></a></li>
                    <li><a href="https://scholar.google.es/citations?user=6CsB8k0AAAAJ&hl=en"><i class="fa fa-graduation-cap"></i></a></li>
		    <div class="item">
                    <h3 class="title"> <span class="place"><a href="assets/cv/CV.pdf">cv</a></span> <span class="year"></span></h3>
                    </div><!--//item-->

                    <!--<li><a href="mailto:xialei@cvc.uab.es"><i class="fa fa-paper-plane"></i></a></li>-->
                    <!--<li class="last-item"><a href="https://www.reddit.com/user/pribaf/"><i class="fa fa-reddit"></i></a></li>-->
                </ul> 
            </div><!--//profile-->
            <div class="profile-content pull-right" style="vertical-align: middle">
                <a href="http://www.cvc.uab.es"><img src="assets/images/cvc.svg" alt="CVC" width="150px" style="margin: 40px 50px"/></a>
                <a href="http://www.uab.cat"><img src="assets/images/uab.svg" alt="UAB" width="150px" style="margin-top: 20px"/></a>
                <a href="http://www.zzu.edu.cn/"><img src="assets/images/1zzu.png" alt="ZZU" width="100px" style="margin-top: 20px"/></a>
            </div> <!--//institution-->
        </div><!--//container-->
    </header><!--//header-->
    
    <div class="container sections-wrapper">
        <div class="row">
            <div class="primary col-md-8 col-sm-12 col-xs-12">
                
    		
		  <section class="experience section">
                    <div class="section-inner">
                        <h2 class="heading">Latest News</h2>
                        <div class="content">
			    <div class="item">
                              <h3 class="title"> 1 papers is accpted by CVIU.
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> 1 papers is accpted by IJCV.
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> 1 papers is accpted by ECCV2020.
                            </div><!--//item-->


			    <div class="item">
                              <h3 class="title"> 1 papers is submited to  Arxiv.
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> 2 papers are accepted by CVPR2020.
                            </div><!--//item-->


			    <div class="item">
                              <h3 class="title"> Intern in IIAI, UAE.
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> 1 paper is accepted by ACM-MM2019, Nice, France.
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> 1 paper is accepted by NIPS2018, Montréal, CANADA. 
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> 1 paper is accepted by ECCV2018,  Munich, Germany. 
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> Attended the international conference CVPR 2018, Salt Lake City, US. 
                            </div><!--//item-->
                            

			    <div class="item">
                              <h3 class="title"> 1 paper is accepted by CVPR2018,  Salt Lake City,  US. 
                            </div><!--//item-->
                            

			   <!-- <div class="item"> <h3 class="title"> Invited Talk: Lifelong Learning Seminar - <span class="place"><a href="http://www.cvc.uab.es/?p=3622"> Link </a></span> <span class="year"></span></h3>
			    </div> --> <!--//item-->


			    <div class="item">
                              <h3 class="title"> Attended the international conference ICLR 2017, Toulon, France. 
                            </div><!--//item-->

			    <div class="item">
                              <h3 class="title"> 1 paper is accepted at a workshop NIPS 2016, Barcelona, Spain. 
                            </div><!--//item-->
			    
                            
                        </div><!--//content-->  
                    </div><!--//section-inner-->                 
                </section><!--//section-->

		<section class="publications section">
                    <div class="section-inner">
                        <h2 class="heading">Conferences</h2>
                        <div class="content">



                        <!-- ECCV2020 -->
                        <div class="item">
                                <h3 class="title">GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images</h3>
                                <!--<h3 class="desc"> <a href="https://arxiv.org/pdf/2003.02567.pdf"> [PDF] </a> <a href="https://arxiv.org/pdf/2003.02567.pdf"> [Code] </a> </h3> -->
                                <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/2003.02567.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://arxiv.org/pdf/2003.02567.pdf"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                    
                                <p class="authors"><strong>Authors:</strong>Lei Kang, Pau Riba, Yaxing Wang, Marçal Rusiñol, Alicia Fornés, Mauricio Villegas</p>
                                <p class="conference">ECCV2020</p>
                                <p id="GANwriting" class="abstract" style="display: none;"><strong>Abstract:</strong> Although current image generation methods have reached impressive quality levels, they are still unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing by hand, a great variability is observed across different writers, and even when analyzing words scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a step closer to producing realistic and varied artificially rendered handwritten words. We propose a novel method that is able to produce credible handwritten word images by conditioning the generative process with both calligraphic style features and textual content. Our generator is guided by three complementary learning objectives: to produce realistic images, to imitate a certain handwriting style and to convey a specific textual content. Our model is unconstrained to any predefined vocabulary, being able to render whatever input word. Given a sample writer, it is also able to mimic its calligraphic features in a few-shot setup. We significantly advance over prior art and demonstrate with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically produced images.</p>
                                <p><a class="more-link" onclick="myFunction('GANwriting')" href="javascript:void(0);"><i id="GANwriting" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->

                        <!-- CVPR2020 -->
                        <div class="item">
                                <h3 class="title">Semi-supervised Learning for Few-shot Image-to-Image Translation</h3>
                                <!--<h3 class="desc"> <a href="https://arxiv.org/pdf/2003.13853.pdf"> [PDF] </a> <a href="https://github.com/yaxingwang/SEMIT"> [Code] </a> </h3> -->
                                <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/2003.13853.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://github.com/yaxingwang/SEMIT"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                    
                                <p class="authors"><strong>Authors:</strong>Yaxing Wang, Salman Khan, Abel Gonzalez-Garcia, Joost van de Weijer, Fahad Shahbaz Khan</p>
                                <p class="conference">CVPR2020</p>
                                <p id="SEMIT" class="abstract" style="display: none;"><strong>Abstract:</strong> In the last few years, unpaired image-to-image translation has witnessed remarkable progress.  Although the latest methods are able to generate realistic images, they crucially rely on a large number of labeled images.  Recently, some methods have tackled the challenging setting of few-shot image-to-image translation, reducing the labeled data requirements for the target domain during inference.  In this work, we go one step further and reduce the amount of required labeled data also from the source domain during training.  To do so, we propose applying semi-supervised learning via a noise-tolerant pseudo-labeling procedure.  We also apply a cycle consistency constraint to further exploit the information from unlabeled images, either from the same dataset or external.  Additionally, we propose several structural modifications to facilitate the image translation task under these circumstances.  Our  semi-supervised method for few-shot image translation, called SEMIT, achieves excellent results on four different datasets using as little as 10% of the source labels, and matches the performance of the main fully-supervised competitor using only 20% labeled data.</p>
                                <p><a class="more-link" onclick="myFunction('SEMIT')" href="javascript:void(0);"><i id="SEMIT" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->




                        <!-- CVPR2020 -->
                        <div class="item">
                                <h3 class="title">MineGAN: effective knowledge transfer from  GANs to target domains with few images</h3>
                                <!--<h3 class="desc"> <a href="https://arxiv.org/pdf/1912.05270.pdf"> [PDF] </a> <a href="https://github.com/yaxingwang/MineGAN"> [Code] </a> </h3> -->
                                <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1912.05270.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://github.com/yaxingwang/MineGAN"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                    
                                <p class="authors"><strong>Authors:</strong>Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, Joost van de Weijer </p>
                                <p class="conference">CVPR2020</p>
                                <p id="MineGAN" class="abstract" style="display: none;"><strong>Abstract:</strong> One of the attractive characteristics of deep neural networks is their ability to transfer knowledge obtained in one domain to other related domains. As a result, high-quality networks can be trained in domains with relatively little training data. This property has been extensively studied for discriminative networks but has received significantly less attention for generative models.  Given the often enormous effort required to train GANs, both computationally as well as in the dataset collection, the re-use of pretrained GANs is a desirable objective.  We propose a novel knowledge transfer method for generative models based on mining the knowledge that is most beneficial to a specific target domain, either from a single or multiple pretrained GANs.  This is done using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain.  Mining effectively steers GAN sampling towards suitable regions of the latent space, which facilitates the posterior finetuning and avoids pathologies of other methods such as mode collapse and lack of flexibility.  We perform experiments on several complex datasets using various GAN architectures (BigGAN, Progressive GAN) and show that the proposed method, called MineGAN, effectively transfers knowledge to domains with few target images, outperforming existing methods.  In addition, MineGAN can successfully transfer knowledge from multiple pretrained GANs.</p>
                                <p><a class="more-link" onclick="myFunction('MineGAN')" href="javascript:void(0);"><i id="MineGAN" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->



                        <!-- ACM-MM2019 -->
                        <div class="item">
                                <h3 class="title">SDIT: Scalable and Diverse Cross-domain Image Translation</h3>
                                <!--<h3 class="desc"> <a href="https://arxiv.org/pdf/1908.06881.pdf"> [PDF] </a> <a href="https://github.com/yaxingwang/SDIT"> [Code] </a> </h3> -->
                                <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1908.06881.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://github.com/yaxingwang/SDIT"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                    
                                <p class="authors"><strong>Authors:</strong>Yaxing Wang, Abel Gonzalez-Garcia, Joost van de Weijer, Luis Herranz </p>
                                <p class="conference">ACM-MM19</p>
                                <p id="SDIT" class="abstract" style="display: none;"><strong>Abstract:</strong>Recently, image-to-image translation research has witnessed remarkable progress. Although current approaches successfully generate diverse outputs or perform scalable image transfer, these properties have not been combined into a single method. To address this limitation, we propose SDIT: Scalable and Diverse image-to-image translation. These properties are combined into a single generator. The diversity is determined by a latent variable which is randomly sampled from a normal distribution.  The scalability is obtained by conditioning the network on the domain attributes. Additionally, we also exploit an attention mechanism that permits the generator to focus on the domain-specific attribute. We empirically demonstrate the performance of the proposed method on face mapping and other datasets beyond faces. </p>
                                <p><a class="more-link" onclick="myFunction('SDIT')" href="javascript:void(0);"><i id="SDIT" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->


                        <!-- NIPS 2018 -->
                        <div class="item">
                                <h3 class="title">Memory Replay GANs: learning to generate images from new categories without forgetting</h3>
                                <!--<h3 class="desc"> <a href="https://arxiv.org/pdf/1805.01677.pdf"> [PDF] </a> <a href="https://github.com/yaxingwang/Transferring-GANs"> [Code] </a> </h3> -->
                                <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1809.02058.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://arxiv.org/abs/1809.02058"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                <p class="authors"><strong>Authors:</strong>Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost van de Weijer, Bogdan Raducanu </p>
                                <p class="conference">NIPS2018</p>
                                <p id="Memory Replay GANs" class="abstract" style="display: none;"><strong>Abstract:</strong>Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories. </p>
                                <p><a class="more-link" onclick="myFunction('Memory Replay GANs')" href="javascript:void(0);"><i id="Memory Replay GANs" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->

                        <!-- ECCV 2018 -->
                        <div class="item">
                                <h3 class="title">Transferring GANs: generating images from limited data</h3>
                                <!--<h3 class="desc"> <a href="https://arxiv.org/pdf/1805.01677.pdf"> [PDF] </a> <a href="https://github.com/yaxingwang/Transferring-GANs"> [Code] </a> </h3> -->
                                <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1805.01677.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://github.com/yaxingwang/Transferring-GANs"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                <p class="authors"><strong>Authors:</strong>Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, Bogdan Raducanu </p>
                                <p class="conference">ECCV2018</p>
                                <p id="transfer GAN" class="abstract" style="display: none;"><strong>Abstract:</strong> Transferring the knowledge of pretrained networks to new domains by means of finetuning is a widely used practice for applications based on discriminative models. To the best of our knowledge this practice has not been studied within the context of generative deep networks. Therefore, we study domain adaptation applied to image generation with generative adversarial networks. We evaluate several aspects of domain adaptation, including the impact of target domain size, the relative distance between source and target domain, and the initialization of conditional GANs. Our results show that using knowledge from pretrained networks can shorten the convergence time and can significantly improve the quality of the generated images, especially when the target data is limited. We show that these conclusions can also be drawn for conditional GANs even when the pretrained model was trained without conditioning. Our results also suggest that density may be more important than diversity and a dataset with one or few densely sampled classes may be a better source model than more diverse datasets such as ImageNet or Places.</p>
                                <p><a class="more-link" onclick="myFunction('transfer GAN')" href="javascript:void(0);"><i id="transfer GAN" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->

                            <!-- CVPR 2018 -->
                            <div class="item">
                                   <h3 class="title">Mix and match networks: encoder-decoder alignment for zero-pair image translation</h3> 
                                   <!--<h3 class="desc"> <a href="https://arxiv.org/abs/1803.03095?context=cs"> [PDF]</a> <a href="https://github.com/xialeiliu/CrowdCountingCVPR18"> [Code] </a> </h3> -->
                                   <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1804.02199.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://github.com/yaxingwang/Mix-and-match-networks"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                <p class="authors"><strong>Authors:</strong> Yaxing Wang, Joost van de Weijer, Luis Herranz</p>
                                <p class="conference">International Conference on Computer Vision and Pattern Recognition (CVPR), 2018</p>
                                <p id="MMNet" class="abstract" style="display: none;"><strong>Abstract:</strong>We address the problem of image translation between domains or modalities for which no direct paired data is available (i.e. zero-pair translation). We propose mix and match networks, based on multiple encoders and decoders aligned in such a way that other encoder-decoder pairs can be composed at test time to perform unseen image translation tasks between domains or modalities for which explicit paired samples were not seen during training. We study the impact of autoencoders, side information and losses in improving the alignment and transferability of trained pairwise translation models to unseen translations. We show our approach is scalable and can perform colorization and style transfer between unseen combinations of domains. We evaluate our system in a challenging cross-modal setting where semantic segmentation is estimated from depth images, without explicit access to any depth-semantic segmentation training pairs. Our model outperforms baselines based on pix2pix and CycleGAN models.</p>
                                <p><a class="more-link" onclick="myFunction('MMNet')" href="javascript:void(0);"><i id="liu2018leveraging" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->

                            <!-- NIPS 2016 Workshop -->
                            <div class="item">
                                   <h3 class="title">Ensembles of generative adversarial networks</h3>
                                   <!--<h3 class="desc"> <a href="https://arxiv.org/abs/1707.08347"> [PDF] </a> <a href="https://github.com/xialeiliu/RankIQA"> [Code] </a> </h3> -->
                                   <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1612.00991.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                    </ul> 
                                <p class="authors"><strong>Authors:</strong>Yaxing Wang, Lichao Zhang, Joost van de Weijer</p>
                                <p class="conference">NIPS 2016 Workshop on Adversarial Training, 2016</p>
                                <p id="EnsemblesGAN" class="abstract" style="display: none;"><strong>Abstract:</strong>Ensembles are a popular way to improve results of discriminative CNNs. The combination of several networks trained starting from different initializations improves results significantly. In this paper we investigate the usage of ensembles of GANs. The specific nature of GANs opens up several new ways to construct ensembles. The first one is based on the fact that in the minimax game which is played to optimize the GAN objective the generator network keeps on changing even after the network can be considered optimal. As such ensembles of GANs can be constructed based on the same network initialization but just taking models which have different amount of iterations. These so-called self ensembles are much faster to train than traditional ensembles. The second method, called cascade GANs, redirects part of the training data which is badly modeled by the first GAN to another GAN. In experiments on the CIFAR10 dataset we show that ensembles of GANs obtain model probability distributions which better model the data distribution. In addition, we show that these improved results can be obtained at little additional computational cost. </p>
                                <p><a class="more-link" onclick="myFunction('EnsemblesGAN')" href="javascript:void(0);"><i id="EnsemblesGAN" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->


                                                        
                        </div><!--//content-->  
                    </div><!--//section-inner-->                 
                </section><!--//section-->

                <section class="publications section">
                    <div class="section-inner">
                        <h2 class="heading">Journals</h2>
                        <div class="content">
                            <div class="item">

                        <!-- CVIU -->
                        <div class="item">
                                <h3 class="title">Controlling biases and diversity in diverse image-to-image translation</h3>
                                <!--<h3 class="desc"> <a href="https://arxiv.org/pdf/1907.09754.pdf"> [PDF] </a> <a href="https://arxiv.org/pdf/1907.09754.pdf"> [Code] </a> </h3> -->
                                <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1907.09754.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://arxiv.org/pdf/1907.09754.pdf"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                    
                                <p class="authors"><strong>Authors:</strong>Yaxing Wang, Abel Gonzalez-Garcia, Luis Herranz, Joost van de Weijer</p>
                                <p class="conference">CVIU</p>
                                <p id="UNIT" class="abstract" style="display: none;"><strong>Abstract:</strong> The task of unpaired image-to-image translation is highly challenging due to the lack of explicit cross-domain pairs of instances.  We consider here diverse image translation (DIT), an even more challenging setting in which an image can have multiple plausible translations.  This is normally achieved by explicitly disentangling content and style in the latent representation and sampling different styles codes while maintaining the image content.  Despite the success of current DIT models, they are prone to suffer from bias.  In this paper, we study the problem of bias in image-to-image translation.  Biased datasets may add undesired changes (e.g. change gender or race in face images) to the output translations as a consequence of the particular underlying visual distribution in the target domain.  In order to alleviate the effects of this problem we propose the use of semantic constraints that enforce the preservation of desired image properties.  Our proposed model is a step towards unbiased diverse image-to-image translation (UDIT), and results in less unwanted changes in the translated images while still performing the wanted transformation.  Experiments on several heavily biased datasets show the effectiveness of the proposed techniques in different domains such as faces, objects, and scenes.</p>
                                <p><a class="more-link" onclick="myFunction('UNIT')" href="javascript:void(0);"><i id="UNIT" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->


                            <!-- IJCV -->
                            <div class="item">
                                   <h3 class="title">Mix and match networks: cross-modal alignment for zero-pair image-to-image translation</h3> 
                                   <!--<h3 class="desc"> <a href="https://arxiv.org/abs/1803.03095?context=cs"> [PDF]</a> <a href="https://github.com/xialeiliu/CrowdCountingCVPR18"> [Code] </a> </h3> -->
                                   <ul class="social list-inline">
                                        <li><a href="https://arxiv.org/pdf/1903.04294.pdf"><i class="fa fa-file-pdf-o"></i></a></li>
                                        <li><a href="https://github.com/yaxingwang/Mix-and-match-networks"><i class="fa fa-github"></i></a></li>
                                    </ul> 
                                <p class="authors"><strong>Authors:</strong> Yaxing Wang, Joost van de Weijer, Luis Herranz</p>
                                <p class="conference">IJCV</p>
                                <p id="MMNet" class="abstract" style="display: none;"><strong>Abstract:</strong>This paper addresses the problem of infer-ring  unseen  cross-modal  image-to-image  translationsbetween multiple modalities. We assume that only someof the pairwise translations have been seen (i.e. trained)and  infer  the  remaining  unseen  translations  (wheretraining pairs are not available). We propose mix andmatch networks, an approach where multiple encodersand  decoders  are  aligned  in  such  a  way  that  the  de-sired translation can be obtained by simply cascadingthe source encoder and the target decoder, even whenthey have not interacted during the training stage (i.e.unseen).  The  main  challenge  lies  in  the  alignment  ofthe latent representations at the bottlenecks of encoder-decoder  pairs.  We  propose  an  architecture  with  sev-eral  tools  to  encourage  alignment,  including  autoen-coders  and  robust  side  information  and  latent  consis-tency losses. We show the benefits of our approach interms  of  effectiveness  and  scalability  compared  withother pairwise image-to-image translation approaches.We  also  propose  zero-pair  cross-modal  image  transla-tion,  a  challenging  setting  where  the  objective  is  in-ferring  semantic  segmentation  from  depth  (and  vice-versa)  without  explicit  segmentation-depth  pairs,  andonly from two (disjoint) segmentation-RGB and depth-RGB training sets. We observe that a certain part of theshared  information  between  unseen  modalities  mightnot be reachable, so we further propose a variant thatleverages  pseudo-pairs  which  allows  us  to  exploit  thisshared information between the unseen modalities.</p>
                                <p><a class="more-link" onclick="myFunction('MMNet')" href="javascript:void(0);"><i id="liu2018leveraging" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->


                        </div><!--//content-->  
                    </div><!--//section-inner-->                 
                </section><!--//section-->

                <section class="projects section">
                    <div class="section-inner">
                        <h2 class="heading">Projects</h2>
                        <div class="content">
                            <div class="item">
				<p class="type">M2CR</p>
                                <h3 class="title"><a href="https://projets-lium.univ-lemans.fr/m2cr/">Multimodal Multilingual Continuous Representation for Human Language Understanding</a></h3>
                                <p class="advisors"><strong>Organizators:</strong> Joost van de Weiger,  Loïc Barrault,  Yoshua Bengio</p>
                                <p id="tfm" class="summary" style="display: none;"><strong>Abstract:</strong> This project is dedicated to the creation of a unified neural architecture for multimodal and multilingual human language understanding</p>
                                <p><a class="more-link" onclick="myFunction('tfm')" href="javascript:void(0);"><i id="b_tfm" class="fa fa-plus"></i> Find out more</a></p>
                            </div><!--//item-->
                           
                        </div><!--//content-->  
                    </div><!--//section-inner-->                 
                </section><!--//section-->
                
                <section class="experience section">
                    <div class="section-inner">
                        <h2 class="heading">Experience</h2>
                        <div class="content">
                            <div class="item">
                                <h3 class="title">Support researcher - <span class="place"><a href="http://www.cvc.uab.es">Computer Vision Center</a></span> <span class="year">(Sep 2015 - Now)</span></h3>
                                <p>Collaborate with the Learning and Machine Perception (LAMP) groups at different research projects.</p>
                            </div><!--//item-->
                            
                            <div class="item">
                                <h3 class="title">Hands-on DL with MatConvNet - <span class="place"><a href="http://germanros.net/online-courses/hands-on-dl/"> Hands-on DL</a></span> <span class="year">(12. 2015 - 1. 2016)</span></h3>
                                <p>Collaborate with researchers in CVC </p>
                            </div><!--//item-->

                            <div class="item">
                                <h3 class="title">Candidate of PHD - <span class="place"><a href="http://www.uab.cat/"> Universitat Autònoma de Barcelona</a></span> <span class="year">(Sep 2015 - Sep 2019)</span></h3>
                                <p>PhD scholarship.</p>
                            </div><!--//item-->
			    
                            
                        </div><!--//content-->  
                    </div><!--//section-inner-->                 
                </section><!--//section-->

                <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading">Bio</h2>
                        <div class="content">
                            <p>  I am a postdoctoral with Joost van de Weijer at Computer Vision Center (CVC). I received my PhD degree from engineering school at Autonomous University of Barcelona(UAB) in 2020 under the advisement of  Joost van de Weijer. I received my MS degree in signal processing from Zhengzhou University in 2015. I have worked on a wide variety of projects including images for Encoder-decoder, Transfer Learning, Domain Adaptation, Lifelong Learning.</p>

                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->

            </div><!--//primary-->

            <div class="secondary col-md-4 col-sm-12 col-xs-12">
                 <aside class="info aside section">
                    <div class="section-inner">
                        <h2 class="heading sr-only">Basic Information</h2>
                        <div class="content">
                            <ul class="list-unstyled">
                                <li>
                                    <table style="display: inline;">
                                        <tr>
                                            <td>
                                                <i class="fa fa-map-marker"></i><span class="sr-only">Location:</span>
                                                <br> <br> <br>
                                            </td>
                                            <td>
                                                Computer Vision Center<br>
                                                Edifici O, Universitat Autònoma de Barcelona<br>
                                                08193, Bellaterra, Barcelona, Catalunya
                                            </td>
                                        </tr>
                                    </table>
                                </li>
                                <li><i class="fa fa-phone"></i><span class="sr-only">Phone:</span>+34 93 581 1828</li>
                                <li><i class="fa fa-envelope-o"></i><span class="sr-only">Email:</span>yaxing &lt;at&gt; cvc.uab.es</li>
                                <li><i class="fa fa-link"></i><span class="sr-only">Website:</span><a href="http://www.cvc.uab.es/LAMP/">http://www.cvc.uab.es/LAMP/</a></li>
                            </ul>
                        </div><!--//content-->  
                    </div><!--//section-inner-->                 
                </aside><!--//aside-->
                
                <aside class="skills aside section">
                    <div class="section-inner">
                        <h2 class="heading">Skills</h2>
                        <div class="content">
                            <div class="skillset">
                               
                                <div class="item">
                                    <h3 class="level-title">Tensorflow<span class="level-label">Proficient</span></h3>
                                    <div class="level-bar">
                                        <div class="level-bar-inner" data-level="95%">
                                        </div>                                      
                                    </div><!--//level-bar-->                                 
                                </div><!--//item-->

                                <div class="item">
                                    <h3 class="level-title">Python<span class="level-label">Proficient</span></h3>
                                    <div class="level-bar">
                                        <div class="level-bar-inner" data-level="95%">
                                        </div>                                      
                                    </div><!--//level-bar-->                                 
                                </div><!--//item-->
                                
                                <div class="item">
                                    <h3 class="level-title">Matlab&Matconvnet<span class="level-label" data-toggle="tooltip" data-placement="left" data-animation="true" title="You can use the tooltip to add more info...">Expert</span></h3>
                                    <div class="level-bar">
                                        <div class="level-bar-inner" data-level="85%">
                                        </div>                                      
                                    </div><!--//level-bar-->                                 
                                </div><!--//item-->
                                

                                <div class="item">
                                    <h3 class="level-title">Caffe<span class="level-label">Proficient</span></h3>
                                    <div class="level-bar">
                                        <div class="level-bar-inner" data-level="80%">
                                        </div>                                      
                                    </div><!--//level-bar-->                                 
                                </div><!--//item-->


                                <div class="item">
                                    <h3 class="level-title">PyTorch<span class="level-label">Proficient</span></h3>
                                    <div class="level-bar">
                                        <div class="level-bar-inner" data-level="90%">
                                        </div>                                      
                                    </div><!--//level-bar-->                                 
                                </div><!--//item-->

                                <div class="item">
                                    <h3 class="level-title">C, C++<span class="level-label">Used it</span></h3>
                                    <div class="level-bar">
                                        <div class="level-bar-inner" data-level="20%">
                                        </div>                                      
                                    </div><!--//level-bar-->                                 
                                </div><!--//item-->
                                
                                <!--<p><a class="more-link" href="#"><i class="fa fa-external-link"></i> More on Coderwall</a></p> -->
                            </div>              
                        </div><!--//content-->  
                    </div><!--//section-inner-->                 
                </aside><!--//section-->
                               
                <aside class="education aside section">
                    <div class="section-inner">
                        <h2 class="heading">Education</h2>
                        <div class="content">
                            <div class="item">                      
                                <h3 class="title"><i class="fa fa-graduation-cap"></i> PhD in Computer Vision</h3>
                                <h4 class="university">Universitat Autònoma de Barcelona</h4>
                                <h4 class="university">Computer Vision Center</h4>
                                <h4 class="year">(2015-Now)</h4>
                           </div><!--//item-->
                           <div class="item">                      
                                <h3 class="title"><i class="fa fa-graduation-cap"></i> MSc in Signal Processing</h3>
                                <h4 class="university">Zhengzhou University</h4>
                                <h4 class="year">(2012-2015)</h4>
                            </div><!--//item-->

                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section--> 
                            
                <aside class="languages aside section">
                    <div class="section-inner">
                        <h2 class="heading">Languages</h2>
                        <div class="content">
                            <ul class="list-unstyled">
                                <li class="item">
                                    <span class="title"><strong>Chinese:</strong></span>
                                    <span class="level">Native Speaker <br class="visible-xs"/><i class="fa fa-star"></i> <i class="fa fa-star"></i> <i class="fa fa-star"></i> <i class="fa fa-star"></i> <i class="fa fa-star"></i> </span>
                                </li><!--//item-->
                            
                                <li class="item">
                                    <span class="title"><strong>English:</strong></span>
                                    <span class="level">Professional Proficiency <br class="visible-sm visible-xs"/><i class="fa fa-star"></i> <i class="fa fa-star"></i> <i class="fa fa-star"></i> <i class="fa fa-star"></i> <i class="fa fa-star-o" aria-hidden="true"></i></span>
                                </li><!--//item-->
			
                            </ul>
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section-->



                <aside class="habits aside section">
                    <div class="section-inner">
                        <h2 class="heading">Habits</h2>
                        <div class="content">
                            <p>The key parts of my spare time are working out, which is good for me from boosting my mood to improving my life,  as well as cooking which leads to healthy life and harmonious family. Dance, my favorite sport,  offers a way to improve strength and flexibility, which helps keep muscles and joints healthy. Shaking body with the melody of music will express my unknown sides. Besides, I also enjoy playing basketball to make new friends with different backgrounds and cultures </p>
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section-->





            </div><!--//secondary-->    
        </div><!--//row-->
    </div><!--//masonry-->

    <!-- ******GO TO TOP****** -->
    <button onclick="topFunction()" id="go2top" title="Go to top"><i class="fa fa-address-card" style="font-size: 2em"></i></button> 

    <!-- ******FOOTER****** --> 
    <footer class="footer">
        <div class="container text-center">
                <small class="copyright">Designed with <i class="fa fa-heart"></i> by <a href="http://themes.3rdwavemedia.com" target="_blank">3rd Wave Media</a> for developers under the <a class="dotted-link" href="http://creativecommons.org/licenses/by/3.0/" target="_blank">Creative Commons Attribution 3.0 License</a> <a href="http://themes.3rdwavemedia.com/website-templates/free-responsive-website-template-for-developers/" target="_blank"><i class="fa fa-download"></i></a></small>
        </div><!--//container-->
    </footer><!--//footer-->

    <!-- Javascript -->          
    <script type="text/javascript" src="assets/plugins/jquery-1.11.1.min.js"></script>
    <script type="text/javascript" src="assets/plugins/jquery-migrate-1.2.1.min.js"></script>
    <script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>    
    <script type="text/javascript" src="assets/plugins/jquery-rss/dist/jquery.rss.min.js"></script> 
    <script>
	function myFunction(id) {
	    var x = document.getElementById(id);
            var b = document.getElementById("b_".concat(id));
	    if (x.style.display === 'none') {
		x.style.display = 'block';
                b.className = "fa fa-minus"
	    } else {
		x.style.display = 'none';
		b.className = "fa fa-plus"
	    }
	}
    </script>
    <!-- GO TO TOP -->
    <script>
    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
        if (document.body.scrollTop > 2000 || document.documentElement.scrollTop > 2000) {
            document.getElementById("go2top").style.display = "block";
        } else {
            document.getElementById("go2top").style.display = "none";
        }
    }

    // When the user clicks on the button, scroll to the top of the document
    function topFunction() {
        document.body.scrollTop = 0; // For Chrome, Safari and Opera
        document.documentElement.scrollTop = 0; // For IE and Firefox
    } 
    </script>
    <!-- github activity plugin -->
    <script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/mustache.js/0.7.2/mustache.min.js"></script>
    <script type="text/javascript" src="http://caseyscarborough.github.io/github-activity/github-activity-0.1.0.min.js"></script>
    <!-- custom js -->
    <script type="text/javascript" src="assets/js/main.js"></script>            
</body>
</html> 

