<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0023)https://jonbarron.info/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    subtitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="https://jonbarron.info/seal_icon.png">
  <title>Kai Wang</title>
  
  <link href="./QingnanFan_files/css" rel="stylesheet" type="text/css">
  <script charset="utf-8" src="chrome-extension://jgphnjokjhjlcnnajmfjlacjnjkhleah/js/btype.js"></script></head>

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <p align="center">
          <name>Kai Wang</name>
        </p>
        <p>
        I'm now working as a PostDoc in <a href="http://www.cvc.uab.es/">LAMP group</a> at Computer Vision Center (CVC), UAB. 
        Before that I graduate as a PhD under the supervision of  <a href="https://www.linkedin.com/in/joost-van-de-weijer-88084b57/">Joost van de Weijer</a> in 2022. 
        I received my master degree in image processing from Jilin University in 2017 and the bachelor degree from Jilin University in 2014. 
        I have worked on a wide variety of projects including Diffusion Models, Continual Learning and Vision Transformers.
        Now I am mainly working on multiple projects on diffusion models and 
        advising several PhD students on related topics.
        <!-- I will join the  <a href="https://www.gbu.edu.cn/?lang=en">Great Bay University</a> as an Assistant Professor  -->
        <!-- from 2025 Spring. -->
         
        <p align="center">
          <a href="mailto:kwang@cvc.uab.es">Email</a> &nbsp/&nbsp
          <!-- <a href="./QingnanFan_files/qingnan_cv.pdf">CV</a> &nbsp/&nbsp -->
          <!-- <a href="./QingnanFan_files/Qingnan-bio.txt">Biography</a> &nbsp/&nbsp -->
          <a href="https://scholar.google.com/citations?user=j14vd0wAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/kai-wang-43129b1b7/">LinkedIn</a> &nbsp/&nbsp
          <a href="https://twitter.com/Alvaro_kai93">Twitter</a> &nbsp/&nbsp
          <a href="https://github.com/wangkai930418">Github</a> &nbsp/&nbsp
          <a href="https://github.com/wangkai930418/awesome-diffusion-categorized">Awesome Diffusion</a>
        </p>
        </td>
        <td width="33%">
        <img src="figs/kai.jpg" width="150" alt="" style="border-style: none" align="middle">
        </td>
      </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td width="100%" valign="middle">
            <strong><heading>News</heading></strong>
            <p>
              <p>2025-01: 2 paper accepted by ICLR 2025, including one Spotlight paper 1Prompt1Story (<5%).</p>
              <p>2025-01: We are looking for PostDocs to join <a href="https://groups.google.com/g/ml-news/c/penSOI3751Y?pli=1">LAMP group</a> working on Diffusion Models. </p>
              <p>2024-10: 1 paper accepted by IEEE TGRS </p>
              <p>2024-10: 1 paper accepted by WACV 2025 </p>
              <p>2024-09: I will serve as an on-site organizer for the first <a href="https://sites.google.com/view/avgenl">AVGenL workshop</a> in ECCV 2024. </p>
              <p>2024-09: 1 paper accepted by NeurIPS 2024 </p>
              <p>2024-08: Give a live presentation online on the <a href="https://course.zhidx.com/c/M2E5Y2YzODMyZDc0NTJjMDkzNjM=">course.zhidx.com</a> platform  </p>
              <p>2024-07: 2 papers accepted in ECCV, 1 paper in IEEE Transactions on Intelligent Vehicles, 1 paper in ECAI</p>
              <p>2024-03: 1 paper accepted in CVPR 2024 AI4CC workshop, 1 paper in ICME 2024 </p>
              <p>2024-01: 1 paper accepted by Knowledge-Based Systems </p>
              <p>2023-10: 1 paper accepted in WACV 2024 </p>
              <p>2023-09: 1 paper accepted in NeurIPS 2023 </p>
              <p>2023-04: 1 paper accepted in ICCV 2023 VCL workshop </p>
            </p>
          </td>
        </tr>
        </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
          <strong><heading>Publications</heading></strong>
          <!-- <p>
            My research focus lies in computer graphics, 3D vision, image processing, and human-computer interaction. My recent effort has been spent on pushing the limit of 3D vision and reinforcement learning technologies to implement an intelligent embodied agent in both forms of physical robots and digital humans. 
          </p> -->
        </td>
      </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()"></tr>
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/1prompt1story.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/1prompt1story.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt</papertitle>
        <br>     
        <a href="https://github.com/byliutao">Tao Liu</a>.
        <strong>Kai Wang*</strong>. 
        <a href="https://sen-mao.github.io/">Senmao Li</a>.
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <a href="https://sites.google.com/view/fahadkhans/home">Fahad Khan</a>.
        <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>. 
        <a href="https://yaxingwang.netlify.app/author/yaxing-wang/">Yaxing Wang</a>.
        <a href="https://scholar.google.com.hk/citations?user=6CIDtZQAAAAJ">Jian Yang</a>.
        <a href="https://mmcheng.net/">Ming-Ming Cheng</a>.
        <br>
            <em>ICLR Spotlight</em>, 2025</strong><br>
            <a href="https://byliutao.github.io/1Prompt1Story.github.io/" target="_blank">project page</a>
            /
            <a href="https://arxiv.org/abs/2501.13554v1" target="_blank">arXiv</a>
            <p></p>
            <p> We propose a training-free approach named 1Prompt1Story for consistent text-to-image generations with a single concatenated prompt. Our method is built on the inherent context consistency propoerty of language models. </td>
        </tr>

        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()"></tr>
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/interlcm.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/interlcm.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>InterLCM: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration</papertitle>
        <br>     
        <a href="https://sen-mao.github.io/">Senmao Li</a>.
        <strong>Kai Wang*</strong>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <a href="https://sites.google.com/view/fahadkhans/home">Fahad Khan</a>.
        <a href="https://mmcheng.net/clguo/">Chunle Guo</a>. 
        <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>. 
        <a href="https://yaxingwang.netlify.app/author/yaxing-wang/">Yaxing Wang</a>.
        <a href="https://scholar.google.com.hk/citations?user=6CIDtZQAAAAJ">Jian Yang</a>.
        <a href="https://mmcheng.net/">Ming-Ming Cheng</a>.
        <br>
            <em>ICLR</em>, 2025</strong><br>
            <a href="https://sen-mao.github.io/InterLCM-Page/" target="_blank">project page</a>
            /
            <a href="https://arxiv.org/abs/2502.02215" target="_blank">arXiv</a>
            <p></p>
            <p>  By regarding the lq image as intermediate state of the LCM, this paper propose the method InterLCM, along with extra conditions as visual embeddings and spatial embeddings, for efficient blind face restoration. </td>
        </tr>

        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()"></tr>
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/s2o.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/s2o.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>Conditional Diffusion Model with Spatial-Frequency Refinement for SAR-to-Optical Image Translation</papertitle>
        <br>     
        <strong>Jiang Qin</strong>. 
        <strong>Kai Wang*</strong>. 
        <strong>Bin Zou</strong>. 
        <strong>Lamei Zhang</strong>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <br>
            <em>IEEE TGRS</em>, 2025</strong><br>
            <a href="https://www.researchgate.net/publication/385571765_Conditional_Diffusion_Model_with_Spatial-Frequency_Refinement_for_SAR-to-Optical_Image_Translation" target="_blank">project page</a>
            /
            <a href="https://www.researchgate.net/publication/385571765_Conditional_Diffusion_Model_with_Spatial-Frequency_Refinement_for_SAR-to-Optical_Image_Translation" target="_blank">arXiv</a>
            <p></p>
            <p>  In this paper, we propose an augmented 
              conditional denoising diffusion probabilistic model 
              with spatial-frequency refinement (SFDiff) 
              for high-fidelity S2O image translation.</td>
        </tr>



        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()"></tr>
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/mcti.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/mcti.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>Multi-Class Textual-Inversion Secretly Yields a Semantic-Agnostic Classifier</papertitle>
        <br>     
        <strong>Kai Wang*</strong>. 
        <a href="https://scholar.google.com/citations?user=S1gksNwAAAAJ&hl=en">Fei Yang</a>. 
        <strong>Bogdan Raducanu</strong>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <br>
            <em>WACV</em>, 2025</strong><br>
            <a href="https://github.com/wangkai930418/mc_ti" target="_blank">project page</a>
            /
            <a href="https://arxiv.org/abs/2410.22317" target="_blank">arXiv</a>
            <p></p>
            <p>  We propose Multi-Class textual inversion, which includes
              a discriminative regularization term for the token updating
              process. Using this technique, our method MC-TI achieves
              stronger Semantic-Agnostic Classification while preserving
              the generation capability of these modifier tokens given only
              few samples per category. </td>
        </tr>


        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()"></tr>
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/tome.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/tome.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis</papertitle>
        <br>     
        <strong>Taihang Hu</strong>. 
        <strong>Linxuan Li</strong>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <a href="https://gao-hongcheng.github.io/">Hongcheng Gao</a>.
        <a href="https://sites.google.com/view/fahadkhans/home">Fahad Khan</a>.
        <a href="https://scholar.google.com.hk/citations?user=6CIDtZQAAAAJ">Jian Yang</a>.
        <a href="https://mmcheng.net/">Ming-Ming Cheng</a>.
        <strong>Kai Wang*</strong>. 
        <a href="https://yaxingwang.netlify.app/author/yaxing-wang/">Yaxing Wang</a>.
        <br>
            <em>NeurIPS</em>, 2024</strong><br>
            <a href="https://github.com/hutaiHang/ToMe" target="_blank">project page</a>
            /
            <a href="https://arxiv.org/abs/2411.07132" target="_blank">arXiv</a>
            <p></p>
            <p>  In this paper, we define semantic binding as the task of associating a given object with its attribute, termed attribute binding, or linking it to other related sub-objects, referred to as object binding. 
              We introduce a novel method called Token Merging (ToMe), which enhances semantic binding by aggregating relevant tokens into a single composite token. 
              This ensures that the object, its attributes and sub-objects all share the same cross-attention map.</td>
        </tr>


        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/colorpeel.jpg" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/colorpeel.jpg" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>ColorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement</papertitle>
        <br>     
        <a href="https://scholar.google.com/citations?user=vf7PeaoAAAAJ&hl=en">Muhammad Atif Butt</a>. 
        <strong>Kai Wang*</strong>. 
        <a href="https://www.jvazquez-corral.net/">Javier Vázquez-Corral</a>.
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <br>
            <em>ECCV</em>, 2024</strong><br>
            <a href="https://moatifbutt.github.io/colorpeel/" target="_blank">project page</a>
            /
            <a href="https://arxiv.org/abs/2407.07197" target="_blank">arXiv</a>
            <p></p>
            <p> We propose to generate a series of geometric shapes with target colors to disentangle (or peel off ) the target colors from the shapes. 
              By jointly learning on multiple color-shape images, we found that the method can successfully disentangle the color and shape concepts. </p>
          </td>
        </tr>


        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/ldc.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/ldc.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>Exemplar-free Continual Representation Learning via Learnable Drift Compensation              </papertitle>
        <br>     
        <a href="https://scholar.google.com/citations?user=A2dhwNgAAAAJ&hl=en">Alex Gomez-Villa</a>. 
        <a href="https://scholar.google.com/citations?user=6_aj45AAAAAJ&hl">Dipam Goswami</a>. 
        <strong>Kai Wang*</strong>. 
        <a href="https://scholar.google.com/citations?user=_Fk4YUcAAAAJ&hl=en">Andrew D. Bagdanov</a>. 
        <a href="https://scholar.google.com/citations?user=8yywECgAAAAJ&hl=en">Bartlomiej Twardowski</a>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <br>
            <em>ECCV</em>, 2024</strong><br>
            <a href="https://github.com/alviur/ldc" target="_blank">project page</a>
            /
            <a href="https://arxiv.org/abs/2407.08536" target="_blank">arXiv</a>
            <p></p>
            <p> We propose Learnable Drift Compensation (LDC), which can effectively mitigate drift in any moving backbone, 
              whether supervised or unsupervised. LDC is fast and straightforward to integrate on top of existing continual learning approaches. 
              Furthermore, we showcase how LDC can be applied in combination with self-supervised CL methods, 
              resulting in the first exemplar-free semi-supervised continual learning approach. </p>
          </td>
        </tr>

        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/avit.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/avit.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>AViTMP: A Tracking-Specific Transformer for Single-Branch Visual Tracking</papertitle>
        <br>     
        <a href="https://scholar.google.com/citations?user=BiRPM9AAAAAJ&hl=en&oi=sra">Chuanming Tang</a>. 
        <strong>Kai Wang*</strong>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        Jianlin Zhang, Yongmei Huang.
        <br>
            <em>IEEE Transactions on Intelligent Vehicles</em>, 2024</strong><br>
            <!-- <a href="https://github.com/wangkai930418/DPL" target="_blank">project page</a> -->
            <!-- /
            <a href="./QingnanFan_files/tog_2023_supp.pdf" target="_blank">supp file</a> -->
            <!-- / -->
            <a href="https://arxiv.org/abs/2310.19542" target="_blank">arXiv</a>
            <p></p>
            <p> To tackle the inferior effectiveness of the vanilla ViT, 
                we propose an Adaptive ViT Model Prediction tracker (AViTMP) 
                to bridge the gap between single-branch network and discriminative models. </p>
          </td>
        </tr>

        
        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/grif_dm.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/grif_dm.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>GRIF-DM: Generation of Rich Impression Fonts using Diffusion Models</papertitle>
        <br>     
        <a href="https://scholar.google.com/citations?user=qDw6-z8AAAAJ&hl=en">Lei Kang</a>. 
        <a href="https://scholar.google.com/citations?user=S1gksNwAAAAJ&hl=en">Fei Yang</a>. 
        <strong>Kai Wang*</strong>. 
        <a href="https://scholar.google.com/citations?user=LXq3YYMAAAAJ&hl=en">Mohamed Ali Souibgui</a>. 
        <a href="https://scholar.google.es/citations?user=U5DQ99QAAAAJ&hl=en">Lluis Gomez</a>. 
        <a href="https://scholar.google.es/citations?user=CmV9_PYAAAAJ&hl=es">Alicia Fornés</a>. 
        <a href="https://scholar.google.es/citations?user=ChmX8ogAAAAJ&hl=en">Ernest Valveny</a>. 
        <a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>.
        <br>
            <em>ECAI</em>, 2024</strong><br>
            <!-- <a href="https://github.com/wangkai930418/DPL" target="_blank">project page</a> -->
            <!-- /
            <a href="./QingnanFan_files/tog_2023_supp.pdf" target="_blank">supp file</a> -->
            <!-- / -->
            <a href="https://arxiv.org/" target="_blank">arXiv</a>
            <p></p>
            <p> TIn this paper, we introduce a diffusion-based method, termed GRIF-DM, 
              to generate fonts that vividly embody specific impressions, 
              utilizing an input consisting of a single letter and a set of descriptive impression keywords. 
              The core innovation of GRIF-DM lies in the development of dual cross-attention modules, 
              which process the characteristics of the letters and impression keywords independently but synergistically, 
              ensuring effective integration of both types of information.</p>
          </td>
        </tr>

        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/locinv.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/locinv.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
              <papertitle>LocInv: Localization-aware Inversion for Text-Guided Image Editing</papertitle>
        <br>     
        <a href="https://scholar.google.com/citations?user=BiRPM9AAAAAJ&hl=en&oi=sra">Chuanming Tang</a>. 
        <strong>Kai Wang*</strong>. 
        <a href="https://scholar.google.com/citations?user=S1gksNwAAAAJ&hl=en">Fei Yang</a>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <br>
            <em>CVPR AI4CC workshop</em>, 2024</strong><br>
            <a href="https://github.com/wangkai930418/DPL" target="_blank">project page</a>
            <!-- /
            <a href="./QingnanFan_files/tog_2023_supp.pdf" target="_blank">supp file</a> -->
            /
            <a href="https://arxiv.org/abs/2405.01496" target="_blank">arXiv</a>
            <p></p>
            <p> We address the cross-attention leakage problem in Text-to-Image diffusion model inversions by introducing the localization priors.</p>
          </td>
        </tr>


        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/keypoint_detect.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/keypoint_detect.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <!-- <a href="./QingnanFan_files/siga_2023.pdf"> -->
              <papertitle>Diffusion-based network for unsupervised landmark detection</papertitle>
            <!-- </a> -->
        <br>     
        Tao Wu,
        <strong>Kai Wang*</strong>. 
        <a href="https://scholar.google.com/citations?user=BiRPM9AAAAAJ&hl=en&oi=sra">Chuanming Tang</a>. 
        Jianlin Zhang.
        <br>
            <em>Knowledge-Based Systems</em>, 2024</strong><br>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705124002624" target="_blank">Journal</a>
            <!-- / -->
            <!-- <a href="https://frank-zy-dou.github.io/projects/CASE/index.html" target="_blank">project page</a>
            /
            <a href="https://youtu.be/Cgq6JbQ1VW4" target="_blank">video</a>
            /
            <a href="./QingnanFan_files/siga_2023.bib">bibtex</a> -->
            <p></p>
            <p> We propose a novel diffusion-based network (DBN) for unsupervised landmark detection, which leverages the generation ability of the diffusion models to detect the landmark locations.</p>
          </td>
        </tr>
   
        
        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
        <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="figs/iterinv.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="figs/iterinv.png" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
        </td>
        <td valign="top" width="75%">
        <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                <papertitle>IterInv: Iterative Inversion for Pixel-Level T2I Models</papertitle>
        <!-- </a> -->
        <br>     
        <a href="https://scholar.google.com/citations?user=BiRPM9AAAAAJ&hl=en&oi=sra">Chuanming Tang</a>. 
        <strong>Kai Wang*</strong>. 
        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
        <br>
            <em>ICME</em>, 2024 & NeurIPS 2023 workshop on Diffusion Models </strong><br>
            <a href="https://github.com/Tchuanm/IterInv" target="_blank">project page</a>
            /
            <a href="https://arxiv.org/abs/2310.19540" target="_blank">arXiv</a>
            <!-- /
            <a href="./QingnanFan_files/cvpr_2023.bib">bibtex</a> -->
            <p></p>
            <p>  We develop an iterative inversion (IterInv) technique for the pixel level T2I models and verify IterInv with the open-source DeepFloyd-IF model.</p>
        </td>
        </tr>

        
        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
            <td width="25%">
                <div class="one">
                <div class="two" id="portrait_image" style="opacity: 0;">
                <img src="figs/pocon.png" width="175" alt="" style="border-style: none" align="middle">
                </div>
                <img src="figs/pocon.png" width="175" alt="" style="border-style: none" align="middle">
                </div>
                <script type="text/javascript">
                function portrait_start() {
                document.getElementById('portrait_image').style.opacity = "1";
                }
                function portrait_stop() {
                document.getElementById('portrait_image').style.opacity = "0";
                }
                portrait_stop()
                </script>
            </td>
            <td valign="top" width="75%">
            <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                    <papertitle>Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning</papertitle>
            <!-- </a> -->
            <br>     
            <a href="https://scholar.google.com/citations?user=A2dhwNgAAAAJ&hl=en">Alex Gomez-Villa</a>. 
            <a href="https://scholar.google.com/citations?user=8yywECgAAAAJ&hl=en">Bartlomiej Twardowski</a>. 
            <strong>Kai Wang*</strong>. 
            <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
            <br>
                <em>WACV</em>, 2024 </strong><br>
                <a href="https://github.com/alviur/pocon_wacv2024" target="_blank">project page</a>  
                /
                <a href="https://arxiv.org/abs/2309.06086" target="_blank">arXiv</a>
                <p></p>
                <p> In this continual learning paper, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity).</p>
            </td>
            </tr>

            <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                <td width="25%">
                    <div class="one">
                    <div class="two" id="portrait_image" style="opacity: 0;">
                    <img src="figs/dpl.png" width="175" alt="" style="border-style: none" align="middle">
                    </div>
                    <img src="figs/dpl.png" width="175" alt="" style="border-style: none" align="middle">
                    </div>
                    <script type="text/javascript">
                    function portrait_start() {
                    document.getElementById('portrait_image').style.opacity = "1";
                    }
                    function portrait_stop() {
                    document.getElementById('portrait_image').style.opacity = "0";
                    }
                    portrait_stop()
                    </script>
                </td>
                <td valign="top" width="75%">
                <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                        <papertitle>Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing</papertitle>
                <!-- </a> -->
                <br>     
                <strong>Kai Wang*</strong>. 
                <a href="https://scholar.google.com/citations?user=S1gksNwAAAAJ&hl=en&oi=sra">Fei Yang</a>. 
                <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>. 
                <a href="https://scholar.google.com/citations?user=vf7PeaoAAAAJ&hl=en&oi=sra">Muhammad Atif Butt</a>. 
                <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                <br>
                    <em>NeurIPS</em>, 2023 </strong><br>
                    <a href="https://github.com/wangkai930418/DPL" target="_blank">project page</a>  
                    /
                    <a href="https://arxiv.org/abs/2309.15664" target="_blank">arXiv</a>
                    <p></p>
                    <p> In this paper, we propose Dynamic Prompt Learning (DPL) to force cross-attention maps to focus on correct noun words in the text prompt.
                        By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses,
                        we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions.</p>
                </td>
                </tr>

                <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                    <td width="25%">
                        <div class="one">
                        <div class="two" id="portrait_image" style="opacity: 0;">
                        <img src="figs/scrollnet.gif" width="175" alt="" style="border-style: none" align="middle">
                        </div>
                        <img src="figs/scrollnet.gif" width="175" alt="" style="border-style: none" align="middle">
                        </div>
                        <script type="text/javascript">
                        function portrait_start() {
                        document.getElementById('portrait_image').style.opacity = "1";
                        }
                        function portrait_stop() {
                        document.getElementById('portrait_image').style.opacity = "0";
                        }
                        portrait_stop()
                        </script>
                    </td>
                    <td valign="top" width="75%">
                    <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                            <papertitle>ScrollNet: Dynamic Weight Importance for Continual Learning</papertitle>
                    <!-- </a> -->
                    <br>     
                    <a href="https://scholar.google.com/citations?user=S1gksNwAAAAJ&hl=en&oi=sra">Fei Yang</a>. 
                    <strong>Kai Wang*</strong>. 
                    <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                    <br>
                        <em>ICCV VCL workshop</em>, 2023 </strong><br>
                        <a href="https://github.com/firefyf/scrollnet" target="_blank">project page</a>  
                        /
                        <a href="https://arxiv.org/abs/2308.16567" target="_blank">arXiv</a>
                        <p></p>
                        <p> In this paper, we propose ScrollNet as a scrolling neural network for continual learning. 
                            ScrollNet can be seen as a dynamic network that assigns the ranking of weight importance for each task before data exposure, 
                            thus achieving a more favorable stability-plasticity tradeoff during sequential task learning by reassigning this ranking for different tasks.</p>
                    </td>
                    </tr>

                    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                      <td width="25%">
                          <div class="one">
                          <div class="two" id="portrait_image" style="opacity: 0;">
                          <img src="figs/onering.png" width="175" alt="" style="border-style: none" align="middle">
                          </div>
                          <img src="figs/onering.png" width="175" alt="" style="border-style: none" align="middle">
                          </div>
                          <script type="text/javascript">
                          function portrait_start() {
                          document.getElementById('portrait_image').style.opacity = "1";
                          }
                          function portrait_stop() {
                          document.getElementById('portrait_image').style.opacity = "0";
                          }
                          portrait_stop()
                          </script>
                      </td>
                      <td valign="top" width="75%">
                      <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                              <papertitle>One ring to bring them all: Towards open-set recognition under domain shift</papertitle>
                      <!-- </a> -->
                      <br>     
                        <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>. 
                        <a href="https://scholar.google.com/citations?user=S1gksNwAAAAJ&hl=en&oi=sra">Yaxing Wang</a>. 
                        <strong>Kai Wang*</strong>. 
                        Shangling Jui,
                        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                      <br>
                          <em>Under Review</em>, 2023 </strong><br>
                          <a href="https://github.com/Albert0147/OneRing_SF-OPDA" target="_blank">project page</a>  
                          /
                          <a href="https://arxiv.org/abs/2206.03600" target="_blank">arXiv</a>
                          <p></p>
                          <p> In this paper, we investigate Source-free Open-partial Domain Adaptation (SF-OPDA), which addresses the situation where
                            there exist both domain and category shifts between source and target domains.</p>
                      </td>
                      </tr>

                    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                        <td width="25%">
                            <div class="one">
                            <div class="two" id="portrait_image" style="opacity: 0;">
                            <img src="figs/dwopp.png" width="175" alt="" style="border-style: none" align="middle">
                            </div>
                            <img src="figs/dwopp.png" width="175" alt="" style="border-style: none" align="middle">
                            </div>
                            <script type="text/javascript">
                            function portrait_start() {
                            document.getElementById('portrait_image').style.opacity = "1";
                            }
                            function portrait_stop() {
                            document.getElementById('portrait_image').style.opacity = "0";
                            }
                            portrait_stop()
                            </script>
                        </td>
                        <td valign="top" width="75%">
                        <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                                <papertitle>Positive Pair Distillation Considered Harmful: Continual Meta Metric Learning for Lifelong Object Re-Identification</papertitle>
                        <!-- </a> -->
                        <br>     
                        <strong>Kai Wang*</strong>. 
                        <a href="https://scholar.google.com/citations?user=FO7GyVwAAAAJ&hl=en">Chenshen Wu</a>. 
                        <a href="https://scholar.google.com/citations?user=_Fk4YUcAAAAJ&hl=en">Andrew D. Bagdanov</a>. 
                        <a href="https://mmcheng.net/xliu/">Xialei Liu</a>. 
                        <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>. 
                        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                        <br>
                            <em>BMVC</em>, 2022 </strong><br>
                            <a href="https://github.com/wangkai930418/DwoPP_code" target="_blank">project page</a>  
                            /
                            <a href="https://arxiv.org/abs/2210.01600" target="_blank">arXiv</a>
                            <p></p>
                            <p> To address the incremental Person ReID problem, 
                                we apply continual meta metric learning to lifelong object re-identification. 
                                To prevent forgetting of previous tasks, 
                                we use knowledge distillation and explore the roles of positive and negative pairs. 
                                Based on our observation that the distillation and metric losses are antagonistic, 
                                we propose to remove positive pairs from distillation to robustify model updates.</p>
                        </td>
                        </tr>


                    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                      <td width="25%">
                          <div class="one">
                          <div class="two" id="portrait_image" style="opacity: 0;">
                          <img src="figs/attndistill.png" width="175" alt="" style="border-style: none" align="middle">
                          </div>
                          <img src="figs/attndistill.png" width="175" alt="" style="border-style: none" align="middle">
                          </div>
                          <script type="text/javascript">
                          function portrait_start() {
                          document.getElementById('portrait_image').style.opacity = "1";
                          }
                          function portrait_stop() {
                          document.getElementById('portrait_image').style.opacity = "0";
                          }
                          portrait_stop()
                          </script>
                      </td>
                      <td valign="top" width="75%">
                      <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                              <papertitle>Attention Distillation: self-supervised vision transformer students need more guidance</papertitle>
                      <!-- </a> -->
                      <br>     
                      <strong>Kai Wang*</strong>. 
                      <a href="https://scholar.google.com/citations?user=S1gksNwAAAAJ&hl=en&oi=sra">Fei Yang</a>. 
                      <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                      <br>
                          <em>BMVC</em>, 2022 </strong><br>
                          <a href="https://github.com/wangkai930418/attndistill" target="_blank">project page</a>  
                          /
                          <a href="https://arxiv.org/abs/2210.00944" target="_blank">arXiv</a>
                          <p></p>
                          <p> In this paper, we study knowledge distillation of self-supervised vision transformers (ViT-SSKD). 
                            We show that directly distilling information from the crucial attention mechanism from teacher to student can significantly narrow the performance gap between both. .
                            </p>
                      </td>

                      </tr>
                      <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                        <td width="25%">
                            <div class="one">
                            <div class="two" id="portrait_image" style="opacity: 0;">
                            <img src="figs/attract_disperse.png" width="175" alt="" style="border-style: none" align="middle">
                            </div>
                            <img src="figs/attract_disperse.png" width="175" alt="" style="border-style: none" align="middle">
                            </div>
                            <script type="text/javascript">
                            function portrait_start() {
                            document.getElementById('portrait_image').style.opacity = "1";
                            }
                            function portrait_stop() {
                            document.getElementById('portrait_image').style.opacity = "0";
                            }
                            portrait_stop()
                            </script>
                        </td>
                        <td valign="top" width="75%">
                        <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                                <papertitle>Attracting and Dispersing: A Simple Approach for Source-free Domain Adaptation</papertitle>
                        <!-- </a> -->
                        <br>     
                        <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>. 
                        <strong>Kai Wang*</strong>. 
                        <a href="https://yaxingwang.netlify.app/author/yaxing-wang/">Yaxing Wang</a>. 
                        Shangling Rui,
                        <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                        <br>
                            <em>NeurIPS (Spotlight)</em>, 2022 </strong><br>
                            <a href="https://github.com/Albert0147/AaD_SFDA" target="_blank">project page</a>  
                            /
                            <a href="https://arxiv.org/abs/2205.04183" target="_blank">arXiv</a>
                            <p></p>
                            <p> We propose a simple but effective source-free domain adaptation (SFDA) method. 
                              Treating SFDA as an unsupervised clustering problem and following the intuition that local neighbors in feature space should have more similar predictions than other features, 
                              we propose to optimize an objective of prediction consistency. 
                              This objective encourages local neighborhood features in feature space to have similar predictions while features farther away in feature space have dissimilar predictions, 
                              leading to efficient feature clustering and cluster assignment simultaneously.

                            </p>
                        </td>
                        </tr>


                        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                          <td width="25%">
                              <div class="one">
                              <div class="two" id="portrait_image" style="opacity: 0;">
                              <img src="figs/hcv.png" width="175" alt="" style="border-style: none" align="middle">
                              </div>
                              <img src="figs/hcv.png" width="175" alt="" style="border-style: none" align="middle">
                              </div>
                              <script type="text/javascript">
                              function portrait_start() {
                              document.getElementById('portrait_image').style.opacity = "1";
                              }
                              function portrait_stop() {
                              document.getElementById('portrait_image').style.opacity = "0";
                              }
                              portrait_stop()
                              </script>
                          </td>
                          <td valign="top" width="75%">
                          <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                                  <papertitle>HCV: Hierarchy-Consistency Verification for Incremental Implicitly-Refined Classification</papertitle>
                          <!-- </a> -->
                          <br>     
                          <strong>Kai Wang*</strong>. 
                          <a href="https://mmcheng.net/xliu/">Xialei Liu</a>. 
                          Luis Herranz,
                          <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                          <br>
                              <em>BMVC</em>, 2021 </strong><br>
                              <a href="https://github.com/wangkai930418/HCV_IIRC" target="_blank">project page</a>  
                              /
                              <a href="https://arxiv.org/abs/2110.11148" target="_blank">arXiv</a>
                              <p></p>
                              <p> Current incremental learning methods lack the ability to build a concept hierarchy by associating new concepts to old ones. 
                                A more realistic setting tackling this problem is referred to as Incremental Implicitly-Refined Classification (IIRC), 
                                which simulates the recognition process from coarse-grained categories to fine-grained categories. 
                                To overcome forgetting in this benchmark, we propose Hierarchy-Consistency Verification (HCV) as an enhancement to existing continual learning methods. 
                              </p>
                          </td>
                          </tr>



                          <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                            <td width="25%">
                                <div class="one">
                                <div class="two" id="portrait_image" style="opacity: 0;">
                                <img src="figs/acae.png" width="175" alt="" style="border-style: none" align="middle">
                                </div>
                                <img src="figs/acae.png" width="175" alt="" style="border-style: none" align="middle">
                                </div>
                                <script type="text/javascript">
                                function portrait_start() {
                                document.getElementById('portrait_image').style.opacity = "1";
                                }
                                function portrait_stop() {
                                document.getElementById('portrait_image').style.opacity = "0";
                                }
                                portrait_stop()
                                </script>
                            </td>
                            <td valign="top" width="75%">
                            <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                                    <papertitle>ACAE-REMIND for Online Continual Learning with Compressed Feature Replay</papertitle>
                            <!-- </a> -->
                            <br>     
                            <strong>Kai Wang*</strong>. 
                            <!-- <a href="https://mmcheng.net/xliu/">Xialei Liu</a>.  -->
                            Luis Herranz,
                            <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                            <br>
                                <em>Pattern Recognition Letters</em>, 2021 </strong><br>
                                <!-- <a href="https://github.com/wangkai930418/HCV_IIRC" target="_blank">project page</a>   -->
                                <!-- / -->
                                <a href="https://arxiv.org/abs/2105.08595" target="_blank">arXiv</a>
                                <p></p>
                                <p> We propose an auxiliary classifier auto-encoder (ACAE) module for feature replay at intermediate layers with high compression rates. 
                                  The reduced memory footprint per image allows us to save more exemplars for replay.
                                </p>
                            </td>
                            </tr>


                            <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                              <td width="25%">
                                  <div class="one">
                                  <div class="two" id="portrait_image" style="opacity: 0;">
                                  <img src="figs/cam_zsl.png" width="175" alt="" style="border-style: none" align="middle">
                                  </div>
                                  <img src="figs/cam_zsl.png" width="175" alt="" style="border-style: none" align="middle">
                                  </div>
                                  <script type="text/javascript">
                                  function portrait_start() {
                                  document.getElementById('portrait_image').style.opacity = "1";
                                  }
                                  function portrait_stop() {
                                  document.getElementById('portrait_image').style.opacity = "0";
                                  }
                                  portrait_stop()
                                  </script>
                              </td>
                              <td valign="top" width="75%">
                              <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                                      <papertitle>On Implicit Attribute Localization for Generalized Zero-Shot Learning</papertitle>
                              <!-- </a> -->
                              <br>     
                              <a href="https://www.shiqiyang.xyz/">Shiqi Yang</a>. 
                              <strong>Kai Wang*</strong>. 
                              <!-- <a href="https://mmcheng.net/xliu/">Xialei Liu</a>.  -->
                              Luis Herranz,
                              <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                              <br>
                                  <em>IEEE Signal Processing Letters</em>, 2021 </strong><br>
                                  <!-- <a href="https://github.com/wangkai930418/HCV_IIRC" target="_blank">project page</a>   -->
                                  <!-- / -->
                                  <a href="https://arxiv.org/abs/2103.04704" target="_blank">arXiv</a>
                                  <p></p>
                                  <p>  In this paper, we show that common
                                    ZSL backbones (without explicit attention nor part detection) can
                                    implicitly localize attributes, yet this property is not exploited.
                                  </p>
                              </td>
                              </tr>

                              <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
                                <td width="25%">
                                    <div class="one">
                                    <div class="two" id="portrait_image" style="opacity: 0;">
                                    <img src="figs/sdc.png" width="175" alt="" style="border-style: none" align="middle">
                                    </div>
                                    <img src="figs/sdc.png" width="175" alt="" style="border-style: none" align="middle">
                                    </div>
                                    <script type="text/javascript">
                                    function portrait_start() {
                                    document.getElementById('portrait_image').style.opacity = "1";
                                    }
                                    function portrait_stop() {
                                    document.getElementById('portrait_image').style.opacity = "0";
                                    }
                                    portrait_stop()
                                    </script>
                                </td>
                                <td valign="top" width="75%">
                                <!-- <a href="./QingnanFan_files/cvpr_2023.pdf"> -->
                                        <papertitle>Semantic Drift Compensation for Class-Incremental Learning</papertitle>
                                <!-- </a> -->
                                <br>     
                                <a href="https://www.shiqiyang.xyz/">Lu Yu</a>. 
                                <a href="https://scholar.google.com/citations?user=8yywECgAAAAJ&hl=en">Bartlomiej Twardowski</a>. 
                                <a href="https://mmcheng.net/xliu/">Xialei Liu</a>. 
                                Luis Herranz,
                                <strong>Kai Wang*</strong>. 
                                Yongmei Cheng, 
                                Shangling Rui,
                                <a href="http://lamp.cvc.uab.es/">Joost van de Weijer</a>.
                                <br>
                                    <em>CVPR</em>, 2020 </strong><br>
                                    <a href="https://github.com/yulu0724/SDC-IL" target="_blank">project page</a>  
                                    / 
                                    <a href="https://arxiv.org/abs/2004.00440" target="_blank">arXiv</a>
                                    <p></p>
                                    <p>Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. 
                                      Therefore, we study incremental learning for embedding networks. 
                                      In addition, we propose a new method to estimate the drift, called semantic drift, 
                                      of features and compensate for it without the need of any exemplars. 
                                      We approximate the drift of previous tasks based on the drift that is experienced by current task data.
                                    </p>
                                </td>
                                </tr>
  


<!-- 

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Work experience</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/vivo.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>VIVO</papertitle>
          <br>
          Lead Researcher
          <br>
          2023-now
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/tencent.png" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Tencent AI Lab</papertitle>
          <br>
          Senior Researcher
          <br>
          2021-2023
        </td>
      </tr>
  </table> -->


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Education</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img src="figs/uab_logo.png" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Universitat Autònoma de Barcelona</papertitle>
          <br>
          Postdoctoral Researcher
          <br>
          Supervised by Prof. Joost van de Weijer
          <br>
          2022-2024
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="figs/uab_logo.png" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Universitat Autònoma de Barcelona</papertitle>
          <br>
          Ph.D. student
          <br>
          Supervised by Prof. Joost van de Weijer
          <br>
          2017-2022
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="figs/jilin_univ_logo.jpeg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Jilin University</papertitle>
          <br>
          Undergraduate student and Master student
          <br>
          2010-2017
        </td>
      </tr>
  </table>


  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Research experience</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">



    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/cambridge.jpg" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>University of Cambridge</papertitle>
        <br>
        Visiting Student
        <br>
        Supervised by Prof. Carola-Bibiane Schönlieb
        <br>
        2018
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/msra.png" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>Microsoft Research Asia</papertitle>
        <br>
        Research Intern
        <br>
        Supervised by Gang Hua, Xin Tong, Jiaolong Yang and David Wipf
        <br>
        2016-2018
      </td>
    </tr>
=


  </table>

<!-- 
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Talks</heading></strong>
        <p>APR. 2022: Active 3D scene understanding and its applications
          <br>“三维视觉与智能图形”前沿论坛, 图图名师讲堂, China
        </p>

        <p>OCT. 2021: Visual Localization
          <br>Embodied AI Workshop, Valse, China
        </p>

        <p>JAN. 2019: Deep Learning in Computational Photography
          <br>USC ICT/UW Reality Lab/Berkeley/Stanford/Google/MSR, US
        </p>  

        <p>DEC. 2018: Deep Learning for Single Image Artifact Removal
          <br>ACCV Tutorial 2018, Australia
        </p>

        <p>DEC. 2018: Image Smoothing via Unsupervised Learning
          <br>SIGGRAPH Asia 2018, Japan; GAMES Webinar, China
        </p>

        <p>AUG. 2018: Discovering Unsupervised Learning in Image Processing
          <br>CIA, Cambridge University, UK
        </p>

        <p>JUN. 2018: Revisiting Deep Intrinsic Image Decomposition
          <br>CVPR 2018, USA
        </p>

        <p>NOV. 2015: Interactive Real-time Video Segmentation
          <br>SIGGRAPH Asia 2015, Japan
        </p>
      </td>
    </tr>
    </tbody></table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Awards</heading></strong>
        <p>2022: Tencent Outstanding Contributor</p>
        <p>2020: CCF Doctorial Dissertation Award Nominee (CCF 优博提名)</p>
        <p>2019: Outstanding Academic Achievement Award of Shandong University</p>
        <p>2018: Academic Star Nominee of Shandong University (10/20000)</p>
        <p>2018: National Scholarship</p>
        <p>2016: Outstanding Academic Achievement Award of Shandong University</p>
        <p>2015: Presidential Scholarship of Shandong University (35/20000)
        (Highest honor for students in SDU, only 35 elected among around 20000 candidates)</p>
        <p>2015: National Scholarship</p>
        <p>2015: Pacemaker to Outstanding Graduate Student of Shandong University</p>
      </td>
    </tr>
    </tbody></table> -->


  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <heading>Mentored students</heading>
          <p>Qihang Fang (PhD student in University of Hong Kong)</p>
          <p>Siyan Dong (PostDoc in University of Hong Kong)</p>
          <p>Yingda Yin (PhD student in Peking University)</p>
          <p>Kai Ye (PhD student in Peking University)</p>
          <p>Jiazhao Zhang (PhD student in Peking University)</p>
          <p>Yunze Liu (PhD student in Tsinghua University)</p>
          <p>Yijia Weng (PhD student in Stanford University)</p>
          <p>Jialei Huang (PhD student in Tsinghua University)</p>
          <p>Guanqi Zhan (PhD student in Oxford University)</p>
      </td>
    </tr>
    </tbody></table> -->
  

</body></html>